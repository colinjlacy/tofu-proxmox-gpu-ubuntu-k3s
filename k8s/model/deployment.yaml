---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ministral-llama-server
  namespace: llm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ministral-llama-server
  template:
    metadata:
      labels:
        app: ministral-llama-server
    spec:

      tolerations:
      - key: nvidia.com/gpu
        operator: Equal
        value: "true"
        effect: NoSchedule

      volumes:
        - name: model
          persistentVolumeClaim:
            claimName: ministral-model-pvc

      initContainers:
        - name: download-model
          image: alpine:3.20
          command: ["/bin/sh","-lc"]
          env:
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token

          args:
            - |
              set -eux
              apk add --no-cache curl coreutils
              cd /models

              FILE="Qwen3-0.6B-Q8_0.gguf"
              URL="https://huggingface.co/Qwen/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q8_0.gguf"

              # If a bad file is already on the PVC, remove it and re-download.
              if [ -f "$FILE" ]; then
                magic="$(head -c 4 "$FILE" 2>/dev/null || true)"
                if [ "$magic" != "GGUF" ]; then
                  echo "Existing file is not GGUF (magic=$magic). Removing."
                  rm -f "$FILE"
                fi
              fi

              if [ ! -f "$FILE" ]; then
                echo "Downloading $FILE ..."
                curl -fL --retry 5 --retry-delay 2 \
                  -H "Authorization: Bearer ${HF_TOKEN}" \
                  -o "$FILE" \
                  "$URL"
              fi

              magic="$(head -c 4 "$FILE" 2>/dev/null || true)"
              [ "$magic" = "GGUF" ] || (echo "Downloaded file is not GGUF (magic=$magic)."; exit 1)
          volumeMounts:
            - name: model
              mountPath: /models

      containers:
        - name: llama-server
          image: ghcr.io/ggml-org/llama.cpp:server-cuda
          command: ["/app/llama-server"]
          args:
            - "--model"
            - "/models/Qwen3-0.6B-Q8_0.gguf"
            - "--host"
            - "0.0.0.0"
            - "--port"
            - "8080"
            - "--ctx-size"
            - "4096"
            - "--n-gpu-layers"
            - "999"
          ports:
            - containerPort: 8080
              name: http
          resources:
            limits:
              nvidia.com/gpu: "1"
              cpu: "4"
              memory: "20Gi"
            requests:
              cpu: "2"
              memory: "12Gi"
          volumeMounts:
            - name: model
              mountPath: /models
